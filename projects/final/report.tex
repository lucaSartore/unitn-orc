\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{geometry}
\usepackage{float}
\geometry{margin=1in}

\title{Optimization and Learning for Robot Control: Final Project}
\author{Luca Sartore - 256154}
\date{}


\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Introduction}


This works aims to develop a neural network that approximate the output of an optimal control problem.

The system selected is pretty straightforward, a robot moves on a 1D space. At each iteration the robot
get a cost that is proportional to the squared control input, and to a positional cost.
The objective of the robot is to plan a trajectory with the least possible cost.

The tests where made with two systems, one named ``Simple'' where the robot's velocity is imply proportional
to the control's input, and one named ``Inertia'' where the robot has an inertia, and the 
control input thus become acceleration.

The system is trained using a Critic-Actor paradigm, and the result is evaluated in two main ways:
\begin{itemize}
    \item \textbf{Trajectory:} Here the optimal trajectories are compared with the trajectories obtained by the neural network
    \item \textbf{Computation time} In this test we compare the computational cost of the various approaches used to solve the same problem
\end{itemize}


\section{Creating a dataset using Optimal Control}


\subsection{Solving the optimal control problem}

The first step in the creation of the dataset, was to
solve an optimal control problem. And the first step
to do so is to formulate the optimal control problem

\subsubsection{Optimal control problem formulation}

In the optimal control problem $u$ is the input (in this case one dimensional)
to the system, while x is the state variable, and it's dimensionality
depends on the system tested

$$
\begin{aligned}
    x &= \begin{cases} 
        \begin{bmatrix} p \end{bmatrix} & \text{for ``simple'' system} \\
        \\
        \begin{bmatrix} p \\ v \end{bmatrix} & \text{for ``inertia'' system} 
    \end{cases} \\
    &\text{where:} \\
    &\quad p \text{ is the position} \\
    &\quad v \text{ is the velocity}
\end{aligned}
$$

Then the optimal control problem can be defined in simple therms with the equation underneath.

Notable components are $l$ (the cost function), that depends on the squared input as well as 
a positional cost, as well as $f$ which represent the dynamics of the system (and is 
therefore dependent on the selected system)
$$
\begin{aligned}
\mathop{\text{minimize}}_{X, U} \quad & \sum_{i=0}^{N-1} l(x_i, u_i) \\
\text{subject to} \quad & x_{i+1} = f(x_i, u_i) \quad i = 0 \dots N-1 \\
& x_0 = x_{init} \\
\text{where} \quad& \\
 & l(x,u) = \frac{1}{2} * u^2 + (p - 1.9) (p - 1.0) (p - 0.6) (p + 0.5) (p + 1.2) (p + 2.1) \\
 & f(x) = \begin{cases} 
    \begin{bmatrix} p \end{bmatrix}
    +  
    \begin{bmatrix}
    p + \delta t \cdot u
    \end{bmatrix}
    & \text{for ``simple'' system} \\

    \\

    \begin{bmatrix}
    p \\
    v
    \end{bmatrix}
    +  
    \begin{bmatrix}
    p + \delta t \cdot v + \frac{1}{2} \Delta t^2 \cdot u\\
    v + \Delta t \cdot u
    \end{bmatrix}
    & \text{for ``inertia'' system}

   \end{cases}
\end{aligned}
$$




\subsubsection{Optimization issues}

During the optimization a few issues emerged, and the solutions are reported below.

The first issue was that the solver was biased towards the cost minimum that is found near $0$.
As we can see in figure \ref{fig:suboptimal_sol} from the initial position ($-1.3$)
the solver should converge in $-1.8$ as it is both closer, and lower cost.
However the solver prefer to ``walk uphill'' the cost's gradient and end up in a worst spot.

This is because no initial guess was provided, and the state variables where therefore
initialized at zero, making the solver biased

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./images/suboptimal_solution.png}
\caption{An example on how the solver prefer a suboptimal solution found near $0$ to a better one found near $-1.8$}
\label{fig:suboptimal_sol}
\end{figure}

To solve this issue the state variable where initialized with random values within specific range,
this made it more more likely that the solver selected a different minimum than the one
in zero. However there was still an high variability on where the solver would end up (even with
the initial state being exactly the same), so to create a more accurate dataset
each input point was solved 10 times, and only the best solution was added to the dataset.


\subsection{Dataset creation}

The dataset creation was straightforward.
The input where generated by picking uniform samples within the following ranges:
\begin{itemize}
    \item {-2.15 to 1.95} for the position
    \item {-5 to 5} for the position velocity
\end{itemize}

The dataset size was selected to be 10k (meaning that 100k OCP problem where solved).
The calculation required approximately 45 minutes (for each one of the two systems tested)
on a 6 cores ryzen 5 5600 CPU. The implementation was multithreaded.


\section{Training the critic}

The training of the critic was done using the following parameters:
\begin{itemize}
    \item \textbf{Loss:} L1 loss (L2 and hubble where also tested, but found to be worst)
    \item \textbf{Optimizer:} AdamW with a learning rate of $10^{-4}$
    \item \textbf{LR Scheduler:} The learning rate was scheduled
        so that it was cut in half every time there where 10 iterations without any improvement.
    \item \textbf{Train-Test split} The training set was split 90/10 for training and validation
    \item \textbf{Batch size} 64 (other options where tested, but the difference seem small to none)
    \item \textbf{Iterations} Max iterations where set to 2000 (even tho the the algorithm was
often terminated after about 500 iterations for lack of improvements).


\end{itemize}
The final validation losses works out to be around 1.0 and 3.3 for the ``simple'' and ``inertia'' systems respectively,
while the training time was approximately 1 minute 45 seconds on an RTX 3060 ti GUP

The sizes of the networks where chosen by gradually increasing the size of the various layers
until the improvement was marginal. A similar logic was applied to the number of layers.
A ``shrinking'' architecture (where the layers start wide and sering once they get closer to the output)
was selected, as it is common doing in regression problems.

\subsection{Simple Critic}

The network for the ``simple'' system's critic ended up being a 4 layers FFNN with the respective
layer widths being (1,1024,256,1). All the activation layers are leaky ReLU.

In the figure \ref{fig:simple_critic_fn} we can see how accurate the critic is compared to the ground truth
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./images/simple_system___critic_function.png}
\caption{Simple Critic accuracy compared to ground truth}
\label{fig:simple_critic_fn}
\end{figure}


\subsection{Inertia Critic}

The network for the ``inertia'' system's critic ended up being a 5 layers FFNN with the respective
layer widths being (2,1024,512,256,1). All the activation layers are leaky ReLU.

In the figure \ref{fig:inertia_critic_fn} we can see how accurate the critic is compared to the ground truth
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./images/inertia_system___critic_function.png}
\caption{Inertia Critic accuracy compared to ground truth}
\label{fig:inertia_critic_fn}
\end{figure}


\section{Training the actor}

The actor's training was done by applying the following loss

$$
Loss = l(x,\pi(x)) + V(x,f(x,\pi(x)))
$$

Where $l$ is the cost function, $V$ is the value approximation (i.e. the critic) and $\pi$
is the policy trained (i.e. the actor).
When training using this loss the weights of the critic are kept frozen, while only the
weights of the actor are optimized.

The parameters used during training are the following:

\begin{itemize}
    \item \textbf{Optimizer:} AdamW with a learning rate of $5\cdot10^{-5}$
    \item \textbf{Batch size:} 2048 (other batch sizes where tested, but the difference was minimal)
    \item \textbf{Iterations} 5000 (where each iteration had only one batch with the input state being randomly generated)
\end{itemize}

Initially the training process did not work at all, and the reason turns out to be interesting:
During training the state ended up outside the range that was considered ``valid'' for the critic,
and therefore the output was inconsistent.
To solve this issue the the critic was edited so that the input is clipped if outside the validity range.

The training took approximately 11 seconds on an RTX 3060 ti GPU. The final losses are not reported
here as they are hard to interpret (given that they depends on the value function).
The size and shape of the network was selected in the same way that the critic's one was selected


\subsection{Simple Actor}


The network for the ``simple'' system's actor ended up being a 4 layers FFNN with the respective
layer widths being (1,512,128,1). All the activation layers are leaky ReLU.

In the figure \ref{fig:simple_actor_fn} we can see how accurate the actor is compared to the ground truth

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./images/simple_system___actor_function.png}
\caption{Simple Actor accuracy compared to ground truth}
\label{fig:simple_actor_fn}
\end{figure}

One interesting detail to note in the chart is how the system struggles a bit in the
``discontinuity'' found around $0.7$.
The discontinuity is due to the fact that around that value the optimal
strategy changes from moving towards the minimum around $-1.8$ to moving to the minimum
around $1.8$.

This is potentially solvable using larger networks, but that would undermine the main
advantage of this approach which is speed.

\subsection{Inertia Actor}


The network for the ``inertia'' system's actor ended up being a 5 layers FFNN with the respective
layer widths being (1,1024,256,64,1). All the activation layers are leaky ReLU.


In the figure \ref{fig:inertia_actor_fn} we can see how accurate the actor is compared to the ground truth

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./images/inertia_system___actor_function.png}
\caption{Inertia Actor accuracy compared to ground truth}
\label{fig:inertia_actor_fn}
\end{figure}

\section{Optimal control vs Actor Approximation}

In this section we report some trajectory that have being obtained with optimal control and with 
the actor to show how well the network was able to learn the optimal policy.

\subsection{Simple system trajectories}


We can note how the trajectories look pretty good.
In the first sub-figure of figure \ref{fig:simple_system_trajecotries} we can note how 
there is a small divergence from the steady state reached by the two control methods.


This can be explained by locking at figure \ref{fig:imperfect_min_point} that shows 
how the policy's value function has a local minimum that is slightly shifted
with respect to the real value function. And issue is reflected in the policy learned.

Initially this issue was much larger, and was solved by simply increasing the size of the policy network.
This trick stopped working at a certain point because the limitation started to become the
dataset's size.
\begin{figure}[H]
\centering
\begin{tabular}{cc}
  \includegraphics[width=80mm]{images/simple_system___actor_vs_ground_trough___0.png} &   \includegraphics[width=80mm]{images/simple_system___actor_vs_ground_trough___1.png} \\
  \includegraphics[width=80mm]{images/simple_system___actor_vs_ground_trough___2.png} &   \includegraphics[width=80mm]{images/simple_system___actor_vs_ground_trough___3.png} \\
\multicolumn{2}{c}{\includegraphics[width=65mm]{images/simple_system___actor_vs_ground_trough___4.png} }\\
\end{tabular}
\caption{Trajectory in the ``simple'' system}
\label{fig:simple_system_trajecotries}
\end{figure}



\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./images/issue_with_imperfect_min_point.png}
\caption{Value function around $-1.8$}
\label{fig:imperfect_min_point}
\end{figure}


\subsection{Inertia system trajectories}

\begin{figure}[H]
\centering
\begin{tabular}{cc}
  \includegraphics[width=80mm]{images/inertia_system___actor_vs_ground_trough___0.png} &   \includegraphics[width=80mm]{images/inertia_system___actor_vs_ground_trough___1.png} \\
  \includegraphics[width=80mm]{images/inertia_system___actor_vs_ground_trough___2.png} &   \includegraphics[width=80mm]{images/inertia_system___actor_vs_ground_trough___3.png} \\
  \includegraphics[width=80mm]{images/inertia_system___actor_vs_ground_trough___4.png} &   \includegraphics[width=80mm]{images/inertia_system___actor_vs_ground_trough___5.png} \\
\multicolumn{2}{c}{\includegraphics[width=65mm]{images/inertia_system___actor_vs_ground_trough___6.png} }\\
\end{tabular}
\caption{Trajectory in the ``simple'' system}
\end{figure}





\section{Solving time of different methods}


\end{document}